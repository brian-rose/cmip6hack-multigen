{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental wrangling of the data into well organized xarray datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/tqdm/autonotebook.py:17: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/intake/source/discovery.py:136: FutureWarning: The drivers ['stac-catalog', 'stac-collection', 'stac-item'] do not specify entry_points and were only discovered via a package scan. This may break in a future release of intake. The packages should be updated.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n",
    "import intake\n",
    "# util.py is in the local directory\n",
    "# it contains code that is common across project notebooks\n",
    "# or routines that are too extensive and might otherwise clutter\n",
    "# the notebook design\n",
    "import util "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pangeo-cmip6-ESM Collection with 28691 entries:\n",
       "\t> 10 activity_id(s)\n",
       "\n",
       "\t> 23 institution_id(s)\n",
       "\n",
       "\t> 48 source_id(s)\n",
       "\n",
       "\t> 29 experiment_id(s)\n",
       "\n",
       "\t> 86 member_id(s)\n",
       "\n",
       "\t> 23 table_id(s)\n",
       "\n",
       "\t> 190 variable_id(s)\n",
       "\n",
       "\t> 7 grid_label(s)\n",
       "\n",
       "\t> 28691 zstore(s)\n",
       "\n",
       "\t> 59 dcpp_init_year(s)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_dict = {}\n",
    "if util.is_ncar_host():\n",
    "    col = intake.open_esm_datastore(\"../catalogs/glade-cmip6.json\")\n",
    "else:\n",
    "    col = intake.open_esm_datastore(\"../catalogs/pangeo-cmip6.json\")\n",
    "col_dict[\"CMIP6\"] = col\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adhoc-ipcc-ar-ESM Collection with 44 entries:\n",
       "\t> 3 mip_id(s)\n",
       "\n",
       "\t> 1 activity_id(s)\n",
       "\n",
       "\t> 12 institution_id(s)\n",
       "\n",
       "\t> 12 source_id(s)\n",
       "\n",
       "\t> 1 experiment_id(s)\n",
       "\n",
       "\t> 1 member_id(s)\n",
       "\n",
       "\t> 1 table_id(s)\n",
       "\n",
       "\t> 3 variable_id(s)\n",
       "\n",
       "\t> 44 zstore(s)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = intake.open_esm_datastore(\"../catalogs/adhoc-ipcc-ar.json\")\n",
    "col_dict[\"pre-CMIP6\"] = col\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mip_ids = ['FAR', 'SAR', 'TAR', 'CMIP6']\n",
    "mip_catalog_dict = {}\n",
    "for mip_id in mip_ids:\n",
    "    if mip_id == 'CMIP6':\n",
    "        mip_catalog_dict[mip_id] = \"CMIP6\"\n",
    "    else:\n",
    "        mip_catalog_dict[mip_id] = \"pre-CMIP6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the common target grid axes\n",
    "dlon, dlat = 1., 1.\n",
    "ds_out = xr.Dataset({'lat': (['lat'], np.arange(-90.+dlat/2., 90., dlat)),\n",
    "                     'lon': (['lon'], np.arange(0.+dlon/2., 360., dlon)),})\n",
    "\n",
    "# Regridding function\n",
    "def regrid_to_common(ds, ds_out):\n",
    "    \"\"\"\n",
    "    Regrid from rectilinear grid to common grid\n",
    "    \"\"\"\n",
    "    regridder = xe.Regridder(ds, ds_out, 'bilinear',periodic=True, reuse_weights=True)\n",
    "    return regridder(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rearth = 6.378E6   # radius of Earth in meters\n",
    "# a DataArray that gives grid cell areas on the lat/lon grid (in units of m^2)\n",
    "area = (np.deg2rad(dlat)*Rearth) * (np.deg2rad(dlon)*Rearth*np.cos(np.deg2rad(ds_out.lat))) * xr.ones_like(ds_out.lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#varnames = ['tas','psl','pr','uas','vas']\n",
    "varnames = ['tas', 'pr']\n",
    "time_slice = slice('1981', '2010') # date range consistent with NCEP reanalysis long-term-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For converting units for precip out\n",
    "cm_to_m = 1.e-2\n",
    "rho_water = 1.e3\n",
    "day_in_s = (24.*60.*60.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a1baf474664abf945165d09db74c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.mip_id'\n",
      "\n",
      "--> There will be 16 group(s)\n",
      "Reuse existing file: bilinear_40x48_180x360_peri.nc\n",
      "Reuse existing file: bilinear_24x36_180x360_peri.nc\n",
      "Reuse existing file: bilinear_72x96_180x360_peri.nc\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.mip_id'\n",
      "\n",
      "--> There will be 16 group(s)\n",
      "Reuse existing file: bilinear_40x48_180x360_peri.nc\n",
      "Reuse existing file: bilinear_24x36_180x360_peri.nc\n",
      "Reuse existing file: bilinear_72x96_180x360_peri.nc\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.mip_id'\n",
      "\n",
      "--> There will be 16 group(s)\n",
      "Reuse existing file: bilinear_48x96_180x360_peri.nc\n",
      "Reuse existing file: bilinear_32x64_180x360_peri.nc\n",
      "Reuse existing file: bilinear_56x64_180x360_peri.nc\n",
      "Reuse existing file: bilinear_32x64_180x360_peri.nc\n",
      "Reuse existing file: bilinear_40x48_180x360_peri.nc\n",
      "Reuse existing file: bilinear_73x96_180x360_peri.nc\n",
      "Reuse existing file: bilinear_64x128_180x360_peri.nc\n",
      "Reuse existing file: bilinear_40x48_180x360_peri.nc\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.mip_id'\n",
      "\n",
      "--> There will be 16 group(s)\n",
      "Reuse existing file: bilinear_48x96_180x360_peri.nc\n",
      "Reuse existing file: bilinear_32x64_180x360_peri.nc\n",
      "Reuse existing file: bilinear_56x64_180x360_peri.nc\n",
      "Reuse existing file: bilinear_32x64_180x360_peri.nc\n",
      "Reuse existing file: bilinear_40x48_180x360_peri.nc\n",
      "Reuse existing file: bilinear_73x96_180x360_peri.nc\n",
      "Reuse existing file: bilinear_64x128_180x360_peri.nc\n",
      "Reuse existing file: bilinear_40x48_180x360_peri.nc\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.mip_id'\n",
      "\n",
      "--> There will be 16 group(s)\n",
      "Reuse existing file: bilinear_48x96_180x360_peri.nc\n",
      "Reuse existing file: bilinear_32x64_180x360_peri.nc\n",
      "Reuse existing file: bilinear_56x64_180x360_peri.nc\n",
      "Reuse existing file: bilinear_64x128_180x360_peri.nc\n",
      "Reuse existing file: bilinear_80x96_180x360_peri.nc\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.mip_id'\n",
      "\n",
      "--> There will be 16 group(s)\n",
      "Reuse existing file: bilinear_48x96_180x360_peri.nc\n",
      "Reuse existing file: bilinear_32x64_180x360_peri.nc\n",
      "Reuse existing file: bilinear_56x64_180x360_peri.nc\n",
      "Reuse existing file: bilinear_64x128_180x360_peri.nc\n",
      "Reuse existing file: bilinear_80x96_180x360_peri.nc\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 19 group(s)\n",
      "Reuse existing file: bilinear_160x320_180x360_peri.nc\n",
      "Reuse existing file: bilinear_64x128_180x360_peri.nc\n",
      "Reuse existing file: bilinear_160x320_180x360_peri.nc\n",
      "Reuse existing file: bilinear_64x128_180x360_peri.nc\n",
      "Reuse existing file: bilinear_180x360_180x360_peri.nc\n",
      "Reuse existing file: bilinear_256x512_180x360_peri.nc\n",
      "Reuse existing file: bilinear_143x144_180x360_peri.nc\n",
      "Reuse existing file: bilinear_128x256_180x360_peri.nc\n",
      "Reuse existing file: bilinear_160x320_180x360_peri.nc\n",
      "Reuse existing file: bilinear_90x144_180x360_peri.nc\n",
      "Reuse existing file: bilinear_90x144_180x360_peri.nc\n",
      "Reuse existing file: bilinear_90x144_180x360_peri.nc\n",
      "Reuse existing file: bilinear_192x288_180x360_peri.nc\n",
      "Reuse existing file: bilinear_192x288_180x360_peri.nc\n",
      "Reuse existing file: bilinear_180x288_180x360_peri.nc\n",
      "Reuse existing file: bilinear_180x288_180x360_peri.nc\n",
      "Reuse existing file: bilinear_96x192_180x360_peri.nc\n",
      "Reuse existing file: bilinear_192x288_180x360_peri.nc\n",
      "Reuse existing file: bilinear_80x96_180x360_peri.nc\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 20 group(s)\n",
      "Reuse existing file: bilinear_160x320_180x360_peri.nc\n",
      "Reuse existing file: bilinear_64x128_180x360_peri.nc\n",
      "Reuse existing file: bilinear_160x320_180x360_peri.nc\n",
      "Reuse existing file: bilinear_80x180_180x360_peri.nc\n",
      "Reuse existing file: bilinear_64x128_180x360_peri.nc\n",
      "Reuse existing file: bilinear_180x360_180x360_peri.nc\n",
      "Reuse existing file: bilinear_256x512_180x360_peri.nc\n",
      "Reuse existing file: bilinear_143x144_180x360_peri.nc\n",
      "Reuse existing file: bilinear_128x256_180x360_peri.nc\n",
      "Reuse existing file: bilinear_160x320_180x360_peri.nc\n",
      "Reuse existing file: bilinear_90x144_180x360_peri.nc\n",
      "Reuse existing file: bilinear_90x144_180x360_peri.nc\n",
      "Reuse existing file: bilinear_90x144_180x360_peri.nc\n",
      "Reuse existing file: bilinear_192x288_180x360_peri.nc\n",
      "Reuse existing file: bilinear_192x288_180x360_peri.nc\n",
      "Reuse existing file: bilinear_180x288_180x360_peri.nc\n",
      "Reuse existing file: bilinear_180x288_180x360_peri.nc\n",
      "Reuse existing file: bilinear_96x192_180x360_peri.nc\n",
      "Reuse existing file: bilinear_192x288_180x360_peri.nc\n",
      "Reuse existing file: bilinear_80x96_180x360_peri.nc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds_dict = {}\n",
    "\n",
    "for mip_id in tqdm(mip_ids):\n",
    "    ds_dict[mip_id] = {}\n",
    "    for varname in varnames:\n",
    "        \n",
    "        col = col_dict[mip_catalog_dict[mip_id]]\n",
    "        cat = col.search(experiment_id='historical', \n",
    "                         table_id='Amon', \n",
    "                         variable_id=varname,\n",
    "                         member_id='r1i1p1f1'  # choose first ensemble member only (for now)\n",
    "                        )\n",
    "\n",
    "        dset_dict = cat.to_dataset_dict(zarr_kwargs={'consolidated': True, 'decode_times': True})\n",
    "\n",
    "        ds_dict[mip_id][varname] = {}\n",
    "        for key, ds in dset_dict.items():\n",
    "            if (mip_catalog_dict[mip_id] == 'pre-CMIP6') and (mip_id != key.split(\".\")[-1]): continue\n",
    "            \n",
    "            # rename spatial dimensions if necessary\n",
    "            if ('longitude' in ds.dims) and ('latitude' in ds.dims):\n",
    "                ds = ds.rename({'longitude':'lon', 'latitude': 'lat'})\n",
    "            ds = xr.decode_cf(ds) # Need this temporarily because setting 'decode_times': True appears broken\n",
    "            ds = ds.squeeze() # get rid of member_id (for now)\n",
    "            \n",
    "            # take long-term mean\n",
    "            timeave = ds.sel(time=time_slice).mean(dim='time')\n",
    "            \n",
    "            # modify pre-CMIP6 chunks\n",
    "            if mip_catalog_dict[mip_id] == 'pre-CMIP6':\n",
    "                timeave = timeave.chunk({'lat':timeave['lat'].size, 'lon':timeave['lon'].size})\n",
    "            \n",
    "            # regrid to common grid\n",
    "            ds_new = regrid_to_common(timeave[varname], ds_out)\n",
    "\n",
    "            # Add metadata and apply various corrections\n",
    "            if mip_catalog_dict[mip_id] == 'CMIP6':\n",
    "                # Correct MCM-UA precipitation due to broken units (Ron Stouffer, personal communication)\n",
    "                if ('MCM-UA' in ds.attrs['parent_source_id']) and (varname == 'pr'):\n",
    "                    # convert from cm/day to kg/m^2/s\n",
    "                    ds_new *= (cm_to_m * rho_water / day_in_s)\n",
    "                    \n",
    "                # TEMPORARY FIX: Correct BCC-ESM1 and CanESM5 which inexplicably have latitude flipped\n",
    "                if (\"BCC-ESM1\" in key) or (\"CanESM5\" in key):\n",
    "                    ds_new['lat'].values = ds_new['lat'].values[::-1]\n",
    "\n",
    "                ds_new.attrs['name'] = ds.attrs['source_id']\n",
    "                \n",
    "            else:\n",
    "                # Maybe chance this at pre-processing stage?\n",
    "                ds_new.attrs['name'] = ds.attrs['institution']\n",
    "            \n",
    "            # drop redundant variables (like \"height: 2m\")\n",
    "            for coord in ds_new.coords:\n",
    "                if coord not in ['lat','lon']:\n",
    "                    ds_new = ds_new.drop(coord)\n",
    "\n",
    "            # Add ensemble as new dimension\n",
    "            ds_new = ds_new.expand_dims({'ensemble': np.array([ds_new.attrs['name']])}, 0)\n",
    "\n",
    "            # Add var as new dimension\n",
    "            #ds_new = ds_new.expand_dims({'var': np.array([varname])}, 0)\n",
    "\n",
    "            # We should keep the metadata!!!\n",
    "            ds_new.attrs['mip_id'] = mip_id\n",
    "            \n",
    "            ds_dict[mip_id][varname][key] = ds_new  # add this to the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FAR': <xarray.Dataset>\n",
       " Dimensions:   (ensemble: 3, lat: 180, lon: 360)\n",
       " Coordinates:\n",
       "   * lat       (lat) float64 -89.5 -88.5 -87.5 -86.5 ... 86.5 87.5 88.5 89.5\n",
       "   * lon       (lon) float64 0.5 1.5 2.5 3.5 4.5 ... 356.5 357.5 358.5 359.5\n",
       "   * ensemble  (ensemble) object 'GFDL' 'GISS' 'UKTR'\n",
       " Data variables:\n",
       "     tas       (ensemble, lat, lon) float64 dask.array<chunksize=(1, 180, 360), meta=np.ndarray>\n",
       "     pr        (ensemble, lat, lon) float64 dask.array<chunksize=(1, 180, 360), meta=np.ndarray>,\n",
       " 'SAR': <xarray.Dataset>\n",
       " Dimensions:   (ensemble: 8, lat: 180, lon: 360)\n",
       " Coordinates:\n",
       "   * lat       (lat) float64 -89.5 -88.5 -87.5 -86.5 ... 86.5 87.5 88.5 89.5\n",
       "   * lon       (lon) float64 0.5 1.5 2.5 3.5 4.5 ... 356.5 357.5 358.5 359.5\n",
       "   * ensemble  (ensemble) object 'CCCma' 'CCSR-NIES' 'CSIRO' ... 'MPIfM' 'NCAR'\n",
       " Data variables:\n",
       "     tas       (ensemble, lat, lon) float64 dask.array<chunksize=(1, 180, 360), meta=np.ndarray>\n",
       "     pr        (ensemble, lat, lon) float64 dask.array<chunksize=(1, 180, 360), meta=np.ndarray>,\n",
       " 'TAR': <xarray.Dataset>\n",
       " Dimensions:   (ensemble: 5, lat: 180, lon: 360)\n",
       " Coordinates:\n",
       "   * lat       (lat) float64 -89.5 -88.5 -87.5 -86.5 ... 86.5 87.5 88.5 89.5\n",
       "   * lon       (lon) float64 0.5 1.5 2.5 3.5 4.5 ... 356.5 357.5 358.5 359.5\n",
       "   * ensemble  (ensemble) object 'CCCma' 'CCSRNIES' 'CSIRO' 'EH4OPYC' 'GFDL'\n",
       " Data variables:\n",
       "     tas       (ensemble, lat, lon) float64 dask.array<chunksize=(1, 180, 360), meta=np.ndarray>\n",
       "     pr        (ensemble, lat, lon) float64 dask.array<chunksize=(1, 180, 360), meta=np.ndarray>,\n",
       " 'CMIP6': <xarray.Dataset>\n",
       " Dimensions:   (ensemble: 19, lat: 180, lon: 360)\n",
       " Coordinates:\n",
       "   * ensemble  (ensemble) object 'BCC-CSM2-MR' 'BCC-ESM1' ... 'MCM-UA-1-0'\n",
       "   * lat       (lat) float64 -89.5 -88.5 -87.5 -86.5 ... 86.5 87.5 88.5 89.5\n",
       "   * lon       (lon) float64 0.5 1.5 2.5 3.5 4.5 ... 356.5 357.5 358.5 359.5\n",
       " Data variables:\n",
       "     tas       (ensemble, lat, lon) float64 dask.array<chunksize=(1, 180, 360), meta=np.ndarray>\n",
       "     pr        (ensemble, lat, lon) float64 dask.array<chunksize=(1, 180, 360), meta=np.ndarray>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a single dictionary whose keys are the MIP id\n",
    "#  Each item in the dict will be a single xr.Dataset combining all data from each MIP generation\n",
    "ens_dict = {}\n",
    "for mip_id in mip_ids:\n",
    "    mipdataset = xr.Dataset()\n",
    "    for varname in varnames:\n",
    "        vardataarray = xr.concat([ds for name, ds in ds_dict[mip_id][varname].items()], dim='ensemble')\n",
    "        mipdataset[varname] = vardataarray\n",
    "    ens_dict[mip_id] = mipdataset\n",
    "ens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
